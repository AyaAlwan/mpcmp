<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Model Predicitons for a <code>glm.cmp</code> Object — predict.cmp • mpcmp</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script>

<!-- sticky kit -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>



<meta property="og:title" content="Model Predicitons for a <code>glm.cmp</code> Object — predict.cmp" />

<meta property="og:description" content="This is a function for obtaining predictions and optionally estimates standard 
errors of those prediction from a fitted COM-Poisson regression object." />
<meta name="twitter:card" content="summary" />



<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">mpcmp</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.1.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/thomas-fung/mpcmp">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Model Predicitons for a <code>glm.cmp</code> Object</h1>
    <small class="dont-index">Source: <a href='https://github.com/thomas-fung/mpcmp/blob/master/R/summarize_extract.R'><code>R/summarize_extract.R</code></a></small>
    <div class="hidden name"><code>predict.cmp.Rd</code></div>
    </div>

    <div class="ref-description">
    
    <p>This is a function for obtaining predictions and optionally estimates standard 
errors of those prediction from a fitted COM-Poisson regression object.</p>
    
    </div>

    <pre class="usage"><span class='co'># S3 method for cmp</span>
<span class='fu'>predict</span>(<span class='no'>object</span>, <span class='kw'>newdata</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>se.fit</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>type</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"link"</span>, <span class='st'>"response"</span>), <span class='no'>...</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>object</th>
      <td><p>an object class 'cmp', obtained from a call to <code>glm.cmp</code>.</p></td>
    </tr>
    <tr>
      <th>newdata</th>
      <td><p>optionally, a data frame in which to look for variables with which to 
predict. If omitted, the fitted linear predictors are used.</p></td>
    </tr>
    <tr>
      <th>se.fit</th>
      <td><p>logical; indicating if standard errors are required.</p></td>
    </tr>
    <tr>
      <th>type</th>
      <td><p>the type of prediction required. The default is 'link' which is the scale 
of the linear predictor i.e., a log scale; the alternative 'response' is on the scale 
of the response variable. The value of this argument can be abbreviated.</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>other arguments passed to or from other methods  (currently unused).</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>If <code>se.fit = FALSE</code>, a vector of predictions.</p>
<p>If <code>se.fit = TRUE</code>, a list with components</p>
<dt>fit</dt><dd><p>Predictions, as for se.fit = FALSE.</p></dd>
<dt>se.fit</dt><dd><p>Estimated standard errors.</p></dd>

    
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>If newdata is omitted the predictions are based on the data used for the fit.</p>
    

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='fu'>data</span>(<span class='no'>takeoverbids</span>)
<span class='no'>M.bids</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='glm.cmp.html'>glm.cmp</a></span>(<span class='no'>numbids</span> ~ <span class='no'>leglrest</span> + <span class='no'>rearest</span> + <span class='no'>finrest</span> + <span class='no'>whtknght</span>
    + <span class='no'>bidprem</span> + <span class='no'>insthold</span> + <span class='no'>size</span> + <span class='no'>sizesq</span> + <span class='no'>regulatn</span>, <span class='kw'>data</span><span class='kw'>=</span><span class='no'>takeoverbids</span>)

<span class='fu'>predict</span>(<span class='no'>M.bids</span>)</div><div class='output co'>#&gt;            1            2            3            4            5            6 
#&gt;  1.006543364  0.259782704  0.760878393  0.173537467  0.186941966  0.738306444 
#&gt;            7            8            9           10           11           12 
#&gt;  0.587648400  0.050091255 -0.239567366  0.149477362 -0.008985166  0.648070767 
#&gt;           13           14           15           16           17           18 
#&gt;  0.793369594  0.356115883  0.538693518 -0.109776734  0.824325666  0.620797665 
#&gt;           19           20           21           22           23           24 
#&gt;  0.429291122 -0.077918847 -0.088044343  0.410661444  0.761308305  1.158495233 
#&gt;           25           26           27           28           29           30 
#&gt;  0.633257569  0.457788118  0.410862952 -0.006169486  0.686457637  0.085798404 
#&gt;           31           32           33           34           35           36 
#&gt;  0.469750487  0.567572559  0.690058930  0.473353687  0.623240057  1.508753303 
#&gt;           37           38           39           40           41           42 
#&gt;  0.038649931  0.268682532  0.487134574  0.335038159  0.062367040  0.498490103 
#&gt;           43           44           45           46           47           48 
#&gt;  0.854004512  0.706526308 -0.135697926  1.081203329  0.841641475  0.823315405 
#&gt;           49           50           51           52           53           54 
#&gt; -0.068573823  0.782012360  0.371931968  0.111688062  1.262561108 -0.346203824 
#&gt;           55           56           57           58           59           60 
#&gt;  0.502833732  0.546215193  0.032135785  1.047183084  0.533185201  0.242203328 
#&gt;           61           62           63           64           65           66 
#&gt;  0.024184227  0.576709549  0.319080475  0.785712892  0.617564189  0.610387336 
#&gt;           67           68           69           70           71           72 
#&gt;  0.616698935  0.331343943  0.511792709 -0.153910591  0.779173798  1.163100498 
#&gt;           73           74           75           76           77           78 
#&gt;  0.471527296  0.953441104  1.005654421  0.612195371 -0.055632693  0.065533148 
#&gt;           79           80           81           82           83           84 
#&gt;  0.027051057  1.059896186  0.105019413  0.831849752  1.149266652  0.128327133 
#&gt;           85           86           87           88           89           90 
#&gt;  0.003910432  0.656095589  0.052910030  0.226928588  0.597819408  0.663104154 
#&gt;           91           92           93           94           95           96 
#&gt;  0.420477873  0.576358730  0.267186865  0.683803099  0.752170336  0.672303570 
#&gt;           97           98           99          100          101          102 
#&gt;  0.251092862  0.453584966  0.738523533  0.164924056  0.132753732  1.384612959 
#&gt;          103          104          105          106          107          108 
#&gt;  0.131484604  0.486728163  0.696341043  0.085229073  1.030029914  0.333338515 
#&gt;          109          110          111          112          113          114 
#&gt;  0.133380110 -0.048423250  0.972534362  0.797832405  0.065122538  0.158178532 
#&gt;          115          116          117          118          119          120 
#&gt;  0.281965261  0.566732691  0.317787422  1.004640786  0.471832204  0.826568009 
#&gt;          121          122          123          124          125          126 
#&gt;  0.007102679  0.554619716  0.831961728  0.319439965 -0.075305323  1.430635603 </div><div class='input'><span class='fu'>predict</span>(<span class='no'>M.bids</span>, <span class='kw'>type</span><span class='kw'>=</span> <span class='st'>"response"</span>)</div><div class='output co'>#&gt;         1         2         3         4         5         6         7         8 
#&gt; 2.7361269 1.2966483 2.1401553 1.1895053 1.2055573 2.0923889 1.7997511 1.0513670 
#&gt;         9        10        11        12        13        14        15        16 
#&gt; 0.7869683 1.1612272 0.9910551 1.9118489 2.2108335 1.4277730 1.7137664 0.8960342 
#&gt;        17        18        19        20        21        22        23        24 
#&gt; 2.2803425 1.8604114 1.5361682 0.9250395 0.9157203 1.5078148 2.1410756 3.1851368 
#&gt;        25        26        27        28        29        30        31        32 
#&gt; 1.8837370 1.5805741 1.5081187 0.9938495 1.9866656 1.0895866 1.5995950 1.7639799 
#&gt;        33        34        35        36        37        38        39        40 
#&gt; 1.9938330 1.6053691 1.8649608 4.5210908 1.0394066 1.3082398 1.6276456 1.3979937 
#&gt;        41        42        43        44        45        46        47        48 
#&gt; 1.0643529 1.6462338 2.3490348 2.0269381 0.8731063 2.9482251 2.3201724 2.2780400 
#&gt;        49        50        51        52        53        54        55        56 
#&gt; 0.9337245 2.1858666 1.4505343 1.1181640 3.5344620 0.7073683 1.6533999 1.7267054 
#&gt;        57        58        59        60        61        62        63        64 
#&gt; 1.0326577 2.8496127 1.7043524 1.2740532 1.0244790 1.7801712 1.3758620 2.1939704 
#&gt;        65        66        67        68        69        70        71        72 
#&gt; 1.8544056 1.8411444 1.8528017 1.3928388 1.6682793 0.8573487 2.1796707 3.1998390 
#&gt;        73        74        75        76        77        78        79        80 
#&gt; 1.6024397 2.5946227 2.7336957 1.8444763 0.9458865 1.0677281 1.0274203 2.8860714 
#&gt;        81        82        83        84        85        86        87        88 
#&gt; 1.1107322 2.2975647 3.1558777 1.1369249 1.0039181 1.9272528 1.0543348 1.2547403 
#&gt;        89        90        91        92        93        94        95        96 
#&gt; 1.8181498 1.9408076 1.5226890 1.7795468 1.3062845 1.9813989 2.1215996 1.9587442 
#&gt;        97        98        99       100       101       102       103       104 
#&gt; 1.2854294 1.5739446 2.0928432 1.1793036 1.1419687 3.9932800 1.1405203 1.6269843 
#&gt;       105       106       107       108       109       110       111       112 
#&gt; 2.0063979 1.0889665 2.8011496 1.3956197 1.1426843 0.9527305 2.6446384 2.2207221 
#&gt;       113       114       115       116       117       118       119       120 
#&gt; 1.0672898 1.1713753 1.3257327 1.7624990 1.3740841 2.7309261 1.6029284 2.2854616 
#&gt;       121       122       123       124       125       126 
#&gt; 1.0071280 1.7412787 2.2978220 1.3763567 0.9274603 4.1813560 </div><div class='input'><span class='fu'>predict</span>(<span class='no'>M.bids</span>, <span class='kw'>se.fit</span><span class='kw'>=</span><span class='fl'>TRUE</span>, <span class='kw'>type</span><span class='kw'>=</span><span class='st'>"response"</span>)</div><div class='output co'>#&gt; $fit
#&gt;         1         2         3         4         5         6         7         8 
#&gt; 2.7361269 1.2966483 2.1401553 1.1895053 1.2055573 2.0923889 1.7997511 1.0513670 
#&gt;         9        10        11        12        13        14        15        16 
#&gt; 0.7869683 1.1612272 0.9910551 1.9118489 2.2108335 1.4277730 1.7137664 0.8960342 
#&gt;        17        18        19        20        21        22        23        24 
#&gt; 2.2803425 1.8604114 1.5361682 0.9250395 0.9157203 1.5078148 2.1410756 3.1851368 
#&gt;        25        26        27        28        29        30        31        32 
#&gt; 1.8837370 1.5805741 1.5081187 0.9938495 1.9866656 1.0895866 1.5995950 1.7639799 
#&gt;        33        34        35        36        37        38        39        40 
#&gt; 1.9938330 1.6053691 1.8649608 4.5210908 1.0394066 1.3082398 1.6276456 1.3979937 
#&gt;        41        42        43        44        45        46        47        48 
#&gt; 1.0643529 1.6462338 2.3490348 2.0269381 0.8731063 2.9482251 2.3201724 2.2780400 
#&gt;        49        50        51        52        53        54        55        56 
#&gt; 0.9337245 2.1858666 1.4505343 1.1181640 3.5344620 0.7073683 1.6533999 1.7267054 
#&gt;        57        58        59        60        61        62        63        64 
#&gt; 1.0326577 2.8496127 1.7043524 1.2740532 1.0244790 1.7801712 1.3758620 2.1939704 
#&gt;        65        66        67        68        69        70        71        72 
#&gt; 1.8544056 1.8411444 1.8528017 1.3928388 1.6682793 0.8573487 2.1796707 3.1998390 
#&gt;        73        74        75        76        77        78        79        80 
#&gt; 1.6024397 2.5946227 2.7336957 1.8444763 0.9458865 1.0677281 1.0274203 2.8860714 
#&gt;        81        82        83        84        85        86        87        88 
#&gt; 1.1107322 2.2975647 3.1558777 1.1369249 1.0039181 1.9272528 1.0543348 1.2547403 
#&gt;        89        90        91        92        93        94        95        96 
#&gt; 1.8181498 1.9408076 1.5226890 1.7795468 1.3062845 1.9813989 2.1215996 1.9587442 
#&gt;        97        98        99       100       101       102       103       104 
#&gt; 1.2854294 1.5739446 2.0928432 1.1793036 1.1419687 3.9932800 1.1405203 1.6269843 
#&gt;       105       106       107       108       109       110       111       112 
#&gt; 2.0063979 1.0889665 2.8011496 1.3956197 1.1426843 0.9527305 2.6446384 2.2207221 
#&gt;       113       114       115       116       117       118       119       120 
#&gt; 1.0672898 1.1713753 1.3257327 1.7624990 1.3740841 2.7309261 1.6029284 2.2854616 
#&gt;       121       122       123       124       125       126 
#&gt; 1.0071280 1.7412787 2.2978220 1.3763567 0.9274603 4.1813560 
#&gt; 
#&gt; $se.fit
#&gt;         1         2         3         4         5         6         7         8 
#&gt; 0.3398654 0.1949361 0.3188484 0.1854664 0.2014942 0.2283130 0.2840500 0.1671532 
#&gt;         9        10        11        12        13        14        15        16 
#&gt; 0.1353031 0.2374757 0.1381251 0.2595772 0.2877561 0.3526128 0.2516810 0.1371015 
#&gt;        17        18        19        20        21        22        23        24 
#&gt; 0.2756863 0.1928675 0.1879901 0.1271586 0.1575508 0.1881086 0.3534741 0.5310113 
#&gt;        25        26        27        28        29        30        31        32 
#&gt; 0.3062101 0.1795926 0.1807902 0.1264654 0.2763478 0.1764203 0.3107715 0.2602157 
#&gt;        33        34        35        36        37        38        39        40 
#&gt; 0.3967425 0.2597417 0.4134087 0.8821137 0.1393417 0.1831607 0.1735358 0.2556732 
#&gt;        41        42        43        44        45        46        47        48 
#&gt; 0.2019467 0.3058691 0.2659618 0.2853154 0.2033638 0.4903590 0.2937582 0.3274080 
#&gt;        49        50        51        52        53        54        55        56 
#&gt; 0.1828802 0.2783493 0.2185279 0.2677876 0.6075340 0.1477758 0.1717275 0.2967678 
#&gt;        57        58        59        60        61        62        63        64 
#&gt; 0.1239862 0.4956008 0.3331529 0.1964423 0.1724266 0.1895130 0.3762587 0.4019291 
#&gt;        65        66        67        68        69        70        71        72 
#&gt; 0.1922726 0.3606026 0.1965984 0.2417947 0.1942024 0.1974756 0.2451939 0.4396414 
#&gt;        73        74        75        76        77        78        79        80 
#&gt; 0.3018186 0.3654345 0.4409668 0.1981385 0.1191026 0.1282110 0.1256179 0.7612324 
#&gt;        81        82        83        84        85        86        87        88 
#&gt; 0.1845890 0.4468627 1.1863745 0.1436444 0.4568276 0.2223317 0.1773803 0.1748991 
#&gt;        89        90        91        92        93        94        95        96 
#&gt; 0.4888382 0.3986556 0.1841253 0.2426558 0.2401388 0.2517396 0.3590897 0.3229142 
#&gt;        97        98        99       100       101       102       103       104 
#&gt; 0.2417180 0.1692063 0.4224213 0.1630600 0.1983972 0.8437062 0.2445048 0.2016312 
#&gt;       105       106       107       108       109       110       111       112 
#&gt; 0.2479430 0.2104933 0.4192925 0.2169172 0.1850519 0.1788137 0.3656226 0.3189791 
#&gt;       113       114       115       116       117       118       119       120 
#&gt; 0.1419270 0.1762643 0.1970247 0.2717152 0.2393524 0.5489669 0.2988470 0.3990902 
#&gt;       121       122       123       124       125       126 
#&gt; 0.1756011 0.3317408 0.4022169 0.3043485 0.1718126 0.8850496 
#&gt; </div><div class='input'>
<span class='no'>newdataframe</span> <span class='kw'>&lt;-</span> <span class='fu'>data.frame</span>(<span class='kw'>bidprem</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>finrest</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>insthold</span> <span class='kw'>=</span> <span class='fl'>0.05</span>,
    <span class='kw'>leglrest</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>rearest</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>regulatn</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>size</span> <span class='kw'>=</span> <span class='fl'>0.1</span>, <span class='kw'>whtknght</span> <span class='kw'>=</span> <span class='fl'>1</span>,
    <span class='kw'>sizesq</span> <span class='kw'>=</span> <span class='fl'>.1</span>^<span class='fl'>2</span>)
<span class='fu'>predict</span>(<span class='no'>M.bids</span>, <span class='kw'>se.fit</span><span class='kw'>=</span><span class='fl'>TRUE</span>, <span class='kw'>newdata</span> <span class='kw'>=</span> <span class='no'>newdataframe</span>, <span class='kw'>type</span><span class='kw'>=</span><span class='st'>"response"</span>)</div><div class='output co'>#&gt; $fit
#&gt;       [,1]
#&gt; 1 1.844874
#&gt; 
#&gt; $se.fit
#&gt;        [,1]
#&gt; 1 0.3870999
#&gt; </div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#details">Details</a></li>
      
      <li><a href="#examples">Examples</a></li>
    </ul>

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Thomas Fung, Aya Alwan, Justin Wishart, Alan Huang.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  

  </body>
</html>

